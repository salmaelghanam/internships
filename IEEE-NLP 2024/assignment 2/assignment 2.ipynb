{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24720d98-c7e3-46e9-94a6-22a53eb5d204",
   "metadata": {
    "id": "24720d98-c7e3-46e9-94a6-22a53eb5d204"
   },
   "source": [
    "# **Arabic**-specific text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df15b97d-c20a-4b4c-bc25-ddcedc029c69",
   "metadata": {
    "id": "df15b97d-c20a-4b4c-bc25-ddcedc029c69",
    "outputId": "21075bdd-5fce-4c8b-8685-975a93214f34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyArabic in c:\\users\\win 11\\appdata\\roaming\\python\\python311\\site-packages (0.6.15)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\win 11\\appdata\\roaming\\python\\python311\\site-packages (from PyArabic) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyArabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e593f2f3-8a2c-4ba0-aec0-469218c4f234",
   "metadata": {
    "id": "e593f2f3-8a2c-4ba0-aec0-469218c4f234"
   },
   "outputs": [],
   "source": [
    "arabic_text = \"ذَهَبَ أَحْمَدُ إِلَى السُّوقِ لِيَشْتَرِيَ بَعْضَ الْفَاكِهَةِ وَالْخُضْرَوَاتِ.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452d0d2-a42f-4c4d-a364-9b8ed29c2742",
   "metadata": {
    "id": "9452d0d2-a42f-4c4d-a364-9b8ed29c2742"
   },
   "source": [
    "# **remove** diacritics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c197ae4-514a-40f4-87ca-c0889a110f00",
   "metadata": {
    "id": "1c197ae4-514a-40f4-87ca-c0889a110f00"
   },
   "source": [
    "Why remove diacritics?\n",
    "\n",
    "Arabic text without diacritics is more common in most use cases (news, social media, etc.).\n",
    "Also, many NLP models, including Stanza’s, work better when diacritics are removed, as they are often not used in everyday written Arabic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812501c-449c-498f-a40b-88b557cd31f1",
   "metadata": {
    "id": "2812501c-449c-498f-a40b-88b557cd31f1",
    "outputId": "82140fe7-c10c-495d-84ba-d1736d007c23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Diacritics:\n",
      "ذَهَبَ أَحْمَدُ إِلَى السُّوقِ لِيَشْتَرِيَ بَعْضَ الْفَاكِهَةِ وَالْخُضْرَوَاتِ\n",
      "\n",
      "Text without Diacritics:\n",
      "ذهب أحمد إلى السوق ليشتري بعض الفاكهة والخضروات\n"
     ]
    }
   ],
   "source": [
    "from pyarabic.araby import strip_tashkeel\n",
    "\n",
    "# Remove Diacritics\n",
    "text_without_diacritics = strip_tashkeel(arabic_text)\n",
    "\n",
    "# Print Results\n",
    "print(\"Text with Diacritics:\")\n",
    "print(arabic_text)\n",
    "print(\"\\nText without Diacritics:\")\n",
    "print(text_without_diacritics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8167a283-8a83-4710-bba2-0caaf586728f",
   "metadata": {
    "id": "8167a283-8a83-4710-bba2-0caaf586728f"
   },
   "source": [
    "strip_tashkeel:\n",
    "A function from pyarabic.araby specifically designed to remove Arabic diacritics (tashkeel).\n",
    "\n",
    "It handles all common diacritics, including Fatha, Kasra, Damma, Shadda, Sukun, and Tanween."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3709587-752a-4782-95f6-2195729209a7",
   "metadata": {
    "id": "b3709587-752a-4782-95f6-2195729209a7"
   },
   "source": [
    "pyarabic remove diacritics easily by strip_tashkeel function without any ambiguous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe736989-6fd5-4d20-a099-92d7d4e9e440",
   "metadata": {
    "id": "fe736989-6fd5-4d20-a099-92d7d4e9e440"
   },
   "source": [
    "# **morphological** analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b05437-f133-4db1-a93b-02a2088909db",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "57a9915b513f4159bf12add4d4e6fc22",
      "86e5eb87b55e410881655bb613514b2b"
     ]
    },
    "id": "a3b05437-f133-4db1-a93b-02a2088909db",
    "outputId": "b33eb730-1f77-4fce-f6d5-9a7e505249e2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a9915b513f4159bf12add4d4e6fc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 18:47:32 INFO: Downloaded file to C:\\Users\\win 11\\stanza_resources\\resources.json\n",
      "2024-11-27 18:47:32 INFO: Downloading default packages for language: ar (Arabic) ...\n",
      "2024-11-27 18:47:33 INFO: File exists: C:\\Users\\win 11\\stanza_resources\\ar\\default.zip\n",
      "2024-11-27 18:47:36 INFO: Finished downloading models and saved to C:\\Users\\win 11\\stanza_resources\n",
      "2024-11-27 18:47:36 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e5eb87b55e410881655bb613514b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 18:47:37 INFO: Downloaded file to C:\\Users\\win 11\\stanza_resources\\resources.json\n",
      "2024-11-27 18:47:39 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "2024-11-27 18:47:39 INFO: Using device: cpu\n",
      "2024-11-27 18:47:39 INFO: Loading: tokenize\n",
      "2024-11-27 18:47:39 INFO: Loading: mwt\n",
      "2024-11-27 18:47:39 INFO: Loading: pos\n",
      "2024-11-27 18:47:40 INFO: Loading: lemma\n",
      "2024-11-27 18:47:40 INFO: Loading: depparse\n",
      "2024-11-27 18:47:40 INFO: Loading: ner\n",
      "2024-11-27 18:47:42 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: ذهب, Lemma: ذَهَب, POS: VERB\n",
      "Word: أحمد, Lemma: أحمد, POS: X\n",
      "Word: إلى, Lemma: إِلَى, POS: ADP\n",
      "Word: السوق, Lemma: سُوق, POS: NOUN\n",
      "Word: ل, Lemma: لِ, POS: CCONJ\n",
      "Word: يشتري, Lemma: اِشتَرَى, POS: VERB\n",
      "Word: بعض, Lemma: بَعض, POS: NOUN\n",
      "Word: الفاكهة, Lemma: فَاكِهَة, POS: NOUN\n",
      "Word: و, Lemma: وَ, POS: CCONJ\n",
      "Word: الخضروات, Lemma: خُضْرَة, POS: NOUN\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Download the Arabic model\n",
    "stanza.download('ar')\n",
    "\n",
    "# Initialize the Arabic pipeline\n",
    "nlp = stanza.Pipeline('ar')\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text_without_diacritics)\n",
    "\n",
    "# Perform morphological analysis\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        print(f\"Word: {word.text}, Lemma: {word.lemma}, POS: {word.upos}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3eea44-cd88-4c6b-a7ae-f007009da9e3",
   "metadata": {
    "id": "7a3eea44-cd88-4c6b-a7ae-f007009da9e3"
   },
   "source": [
    "Stanza provides a pre-trained model for Arabic which has been trained on a large corpus, making it a reliable tool for processing Arabic text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a9a5e-b8a1-4bc3-9f75-81f43ca64705",
   "metadata": {
    "id": "7e5a9a5e-b8a1-4bc3-9f75-81f43ca64705"
   },
   "source": [
    "Stanza is a powerful and convenient tool for Arabic morphological analysis. It provides an easy-to-use pipeline for a wide range of NLP tasks.\n",
    "By using pre-trained models, you can quickly analyze Arabic text without worrying about training the models from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8813859-d468-4414-b249-a0ad45b49e3b",
   "metadata": {
    "id": "e8813859-d468-4414-b249-a0ad45b49e3b"
   },
   "source": [
    "# **handling** dialect manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ebed74-6f74-4e33-896d-3246c26a5c7e",
   "metadata": {
    "id": "f4ebed74-6f74-4e33-896d-3246c26a5c7e",
    "outputId": "e4e6b135-8616-41e4-e7a7-993a604bdb12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: إزايك عامل إيه؟\n",
      "Normalized Text: كيف حالك عامل إيه؟\n"
     ]
    }
   ],
   "source": [
    "# Updated dialect-to-MSA mapping\n",
    "dialect_to_msa = {\n",
    "    \"مفيش\": \"لا يوجد\",   # Egyptian Arabic \"mfeesh\" -> MSA \"laa yoojad\"\n",
    "    \"وين\": \"أين\",         # Levantine Arabic \"wein\" -> MSA \"ayn\"\n",
    "    \"كيفك\": \"كيف حالك\",    # Levantine Arabic \"kayfak\" -> MSA \"kayfa haalik\"\n",
    "    \"إزاي\": \"كيف\",         # Egyptian Arabic \"izay\" -> MSA \"kayfa\"\n",
    "    \"إزايك\": \"كيف حالك\",   # Egyptian Arabic \"izayk\" -> MSA \"kayfa haalik\"\n",
    "    \"عندك\": \"لديك\",       # Egyptian Arabic \"indak\" -> MSA \"ladayk\"\n",
    "}\n",
    "\n",
    "def normalize_dialect(text, lexicon):\n",
    "    tokens = text.split()  # Tokenization by spaces (simplified)\n",
    "    normalized_tokens = [lexicon.get(token, token) for token in tokens]\n",
    "    return \" \".join(normalized_tokens)\n",
    "\n",
    "# Example text in Egyptian Arabic\n",
    "text = \"إزايك عامل إيه؟\"  # Egyptian Arabic: \"How are you? What’s up?\"\n",
    "normalized_text = normalize_dialect(text, dialect_to_msa)\n",
    "print(f\"Original Text: {text}\")\n",
    "print(f\"Normalized Text: {normalized_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b27c9fa-e51a-4c23-ba5c-8faba8db5d88",
   "metadata": {
    "id": "7b27c9fa-e51a-4c23-ba5c-8faba8db5d88"
   },
   "source": [
    "# **English**-specific text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c73419-330b-425a-860e-87b7a0040cd0",
   "metadata": {
    "id": "81c73419-330b-425a-860e-87b7a0040cd0"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84498a2-0247-455a-9a5d-27e60b883123",
   "metadata": {
    "id": "d84498a2-0247-455a-9a5d-27e60b883123",
    "outputId": "23acaa57-ff1e-4470-8759-58a6e29d10e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\win\n",
      "[nltk_data]     11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\win\n",
      "[nltk_data]     11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\win 11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334afa36-ab42-4fb0-bf10-082f7742eecc",
   "metadata": {
    "id": "334afa36-ab42-4fb0-bf10-082f7742eecc"
   },
   "outputs": [],
   "source": [
    "text = \"The CFO confirmed that the FY 2024 budget is ready.The meeting will take place on Mon, 10th Jan. We need to finalize the NDA before the project launch.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c49c7-a2ba-47f1-a854-6c59c1cda83e",
   "metadata": {
    "id": "8c4c49c7-a2ba-47f1-a854-6c59c1cda83e"
   },
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b8bc2e-0d2f-4ff3-831b-c9a839f89848",
   "metadata": {
    "id": "82b8bc2e-0d2f-4ff3-831b-c9a839f89848",
    "outputId": "a5ec72f5-3d74-4bde-c780-54db8b4ca77b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the original text : The CFO confirmed that the FY 2024 budget is ready.The meeting will take place on Mon, 10th Jan. We need to finalize the NDA before the project launch.\n",
      "Stemmed Words:  the cfo confirm that the fy budget is meet will take place on mon we need to final the nda befor the project launch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Stem the words (excluding punctuation)\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens if word.isalpha()]\n",
    "\n",
    "# Display the result\n",
    "print(\"the original text :\",text)\n",
    "print(\"Stemmed Words: \", ' '.join(stemmed_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe12a02-8c6e-4e35-8fa8-17695ef40e5a",
   "metadata": {
    "id": "7fe12a02-8c6e-4e35-8fa8-17695ef40e5a"
   },
   "source": [
    "In this output:\n",
    "\n",
    "\"confirmed\" becomes \"confirm\".\n",
    "\"finalize\" becomes \"final\".\n",
    "\"before\" remains as \"befor\" since it is already close to the root form.\n",
    "\"meeting\" becomes \"meet\".\n",
    "Other words are either left unchanged or shortened to their root forms (e.g., \"CFO\", \"FY\", \"NDA\", \"Mon\" are not changed since they are abbreviations or proper nouns).\n",
    "stemming involves chopping off prefixes or suffixes from words to obtain a common root. the stems may not be invalid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d389d1b5-64f5-4fd5-82fb-d26c61dba78c",
   "metadata": {
    "id": "d389d1b5-64f5-4fd5-82fb-d26c61dba78c"
   },
   "source": [
    "# **lemitization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d292543-1d44-4a6d-b249-37f2a01785c7",
   "metadata": {
    "id": "5d292543-1d44-4a6d-b249-37f2a01785c7",
    "outputId": "8856709f-a319-4e84-9214-834a7dcea28e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words:  The CFO confirm that the FY budget be meeting will take place on Mon We need to finalize the NDA before the project launch\n"
     ]
    }
   ],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# POS tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Function to map NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to NOUN if unknown\n",
    "\n",
    "# Lemmatize the words using correct POS tag\n",
    "lemmatized_words = [\n",
    "    lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags if word.isalpha()\n",
    "]\n",
    "\n",
    "# Display the result\n",
    "print(\"Lemmatized Words: \", ' '.join(lemmatized_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8db1cc4-a9c9-4da4-8dfe-d2db2ba7e4f0",
   "metadata": {
    "id": "b8db1cc4-a9c9-4da4-8dfe-d2db2ba7e4f0"
   },
   "source": [
    "why we use pos with lemitization?\n",
    "Using POS tagging ensures that words like verbs and nouns are processed correctly based on their context, making the lemmatization more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eaf510-766d-4a95-ab4a-98eafac71020",
   "metadata": {
    "id": "65eaf510-766d-4a95-ab4a-98eafac71020"
   },
   "source": [
    "\"confirmed\" is tagged as a verb (VB), and lemmatization changes it to \"confirm\".\n",
    "\n",
    "\"finalize\" is a verb (not a noun), and lemmatization does not change it, since it’s already in its base form.\n",
    "\n",
    "\"meeting\" is a noun, so it stays as \"meeting\".\n",
    "\n",
    "\"launch\" is a noun, so it stays as \"launch\".\n",
    "\n",
    "lemmatization aims for a valid base form through linguistic analysis make it more accurate than stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96825e23-99df-431e-b44d-9d3b7856241c",
   "metadata": {
    "id": "96825e23-99df-431e-b44d-9d3b7856241c"
   },
   "source": [
    "# **handling** abbreviations after lemitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17cfd8f-7a67-4b36-9d6a-811acccc7c82",
   "metadata": {
    "id": "b17cfd8f-7a67-4b36-9d6a-811acccc7c82",
    "outputId": "fa228809-cf24-4b53-8879-1fb3fb928798"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded Text: \n",
      "The Chief Financial Officer confirm that the Fiscal Year budget be meeting will take place on Monday We need to finalize the Non-Disclosure Agreement before the project launch\n"
     ]
    }
   ],
   "source": [
    "abbreviations = {\n",
    "    \"FY\": \"Fiscal Year\",\n",
    "    \"CFO\": \"Chief Financial Officer\",\n",
    "    \"Mon\": \"Monday\",\n",
    "    \"Jan\": \"January\",\n",
    "    \"NDA\": \"Non-Disclosure Agreement\",\n",
    "    \"etc\": \"et cetera\"\n",
    "}\n",
    "def expand_abbreviations(text):\n",
    "    return ' '.join([abbreviations.get(word, word) for word in lemmatized_words])\n",
    "\n",
    "# Expand abbreviations in the text\n",
    "expanded_text = expand_abbreviations(lemmatized_words)\n",
    "\n",
    "# Display the result\n",
    "print(\"Expanded Text: \")\n",
    "print( expanded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f063f-c5ef-4242-b65d-329a95384911",
   "metadata": {
    "id": "555f063f-c5ef-4242-b65d-329a95384911"
   },
   "source": [
    "Abbreviation Dictionary: We define a dictionary abbreviations that maps common abbreviations to their expanded forms (e.g., \"FY\": \"Fiscal Year\").\n",
    "\n",
    "Expanding Abbreviations: The function expand_abbreviations() tokenizes the text, and if an abbreviation is found, it replaces it with its full form using the dictionary. If a word is not an abbreviation (not in the dictionary), it remains unchanged.\n",
    "Result: After running the code, all abbreviations are expanded to their full forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a4a278-3d11-4b25-af1b-f9d68711fd4a",
   "metadata": {
    "id": "f5a4a278-3d11-4b25-af1b-f9d68711fd4a"
   },
   "source": [
    "# **Advanced** text handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2240bf7e-ce32-4dcf-aa50-ff0feaab0fef",
   "metadata": {
    "id": "2240bf7e-ce32-4dcf-aa50-ff0feaab0fef"
   },
   "source": [
    "# **multilingual** processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df8a0c6-b90b-4815-830e-1047d48ad152",
   "metadata": {
    "id": "9df8a0c6-b90b-4815-830e-1047d48ad152"
   },
   "outputs": [],
   "source": [
    "multilingual_text=\"I love learning languages.J'adore apprendre de nouvelles langues.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c10bc-52d8-429e-b57e-2050cd6437b7",
   "metadata": {
    "id": "3d3c10bc-52d8-429e-b57e-2050cd6437b7"
   },
   "source": [
    "NLP Tasks in this:\n",
    "\n",
    "Language Detection: Identify the language of each part of the text.\n",
    "\n",
    "Tokenization: Tokenize the text into individual words.\n",
    "\n",
    "Multilingual Processing: Handle the text differently based on the language detected.\n",
    "\n",
    "(Optional) Translation: Translate the text into a single language if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442a1f13-ea9e-4fa2-b437-1b2bb29a9c4a",
   "metadata": {
    "id": "442a1f13-ea9e-4fa2-b437-1b2bb29a9c4a"
   },
   "source": [
    "1. Language Detection:\n",
    "We'll start by detecting the language of each sentence in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d383f9-64fb-42a7-9265-4fe97a16794c",
   "metadata": {
    "id": "62d383f9-64fb-42a7-9265-4fe97a16794c",
    "outputId": "005fbb44-8164-4bbf-9a21-6d5b47cf4b42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'I love learning languages' ===> Detected Language: en\n",
      "Sentence: 'J'adore apprendre de nouvelles langues' ===> Detected Language: fr\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "# Split text into sentences\n",
    "sentences = multilingual_text.split('.')\n",
    "\n",
    "# Function to detect language\n",
    "def detect_language(multilingual_text):\n",
    "    return detect(multilingual_text)\n",
    "\n",
    "# Detect language for each sentence\n",
    "for sentence in sentences:\n",
    "    if sentence:\n",
    "        language = detect_language(sentence)\n",
    "        print(f\"Sentence: '{sentence}' ===> Detected Language: {language}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0292fbb3-aa4d-45b8-baf7-6bc990f6cadb",
   "metadata": {
    "id": "0292fbb3-aa4d-45b8-baf7-6bc990f6cadb"
   },
   "source": [
    "Explanation of Language Detection:\n",
    "Input Text: \"I love learning languages. J'adore apprendre de nouvelles langues.\"\n",
    "\n",
    "Process: We split the text into sentences and detect the language of each sentence using the langdetect library.\n",
    "\n",
    "Output: The language of each sentence is detected (English, French)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6e6e7f-0680-449f-a5b3-29a0fd8de113",
   "metadata": {
    "id": "7b6e6e7f-0680-449f-a5b3-29a0fd8de113"
   },
   "source": [
    "2. Tokenization\n",
    "Next, we tokenize each sentence based on the detected language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34594ca-46bf-47f2-8da0-b2a9e6eaa85b",
   "metadata": {
    "id": "a34594ca-46bf-47f2-8da0-b2a9e6eaa85b"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca49e3-3491-4c74-8859-83244814ba66",
   "metadata": {
    "id": "e9ca49e3-3491-4c74-8859-83244814ba66"
   },
   "outputs": [],
   "source": [
    "# Load spaCy models for different languages\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "nlp_fr = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae20a6-3593-4971-9dc4-3d52ff9605bd",
   "metadata": {
    "id": "91ae20a6-3593-4971-9dc4-3d52ff9605bd",
    "outputId": "89b1b904-06e5-480c-d0a9-c6ff8663c85e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: en\n",
      "Tokens: ['I', 'love', 'learning', 'languages', '.']\n",
      "Language: fr\n",
      "Tokens: [\"J'\", 'adore', 'apprendre', 'de', 'nouvelles', 'langues', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function for multilingual text\n",
    "def tokenize_multilingual(multilingual_text):\n",
    "    # Ensure proper sentence splitting with multilingual support\n",
    "    sentences = nltk.sent_tokenize(multilingual_text)  # Split into sentences\n",
    "    all_tokens = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()  # Clean the sentence\n",
    "        if sentence:  # Only process non-empty sentences\n",
    "            language = detect_language(sentence)\n",
    "            # Use appropriate spaCy model based on detected language\n",
    "            if language == 'en':\n",
    "                doc = nlp_en(sentence)\n",
    "            elif language == 'fr':\n",
    "                doc = nlp_fr(sentence)\n",
    "            else:\n",
    "                continue  # Skip unsupported languages\n",
    "\n",
    "            tokens = [token.text for token in doc]  # Extract tokens\n",
    "            all_tokens.append((language, tokens))  # Store language and tokens\n",
    "    return all_tokens\n",
    "\n",
    "# Tokenize the multilingual text\n",
    "processed_tokens = tokenize_multilingual(multilingual_text)\n",
    "\n",
    "# Display the tokens\n",
    "for language, tokens in processed_tokens:\n",
    "    print(f\"Language: {language}\")\n",
    "    print(f\"Tokens: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d85e6-41fc-43d3-b033-10509df1a05d",
   "metadata": {
    "id": "679d85e6-41fc-43d3-b033-10509df1a05d"
   },
   "source": [
    "Explanation of Tokenization:\n",
    "Input Text: \"I love learning languages. J'adore apprendre de nouvelles langues.\"\n",
    "\n",
    "Process:\n",
    "The text is split into sentences.\n",
    "Based on the detected language (en, fr), we use the appropriate spaCy model to tokenize each sentence.\n",
    "\n",
    "Output: The tokens (words) are extracted for each sentence in the detected language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f12e8a-e00a-48b5-b5c6-1c184ec97d5a",
   "metadata": {
    "id": "48f12e8a-e00a-48b5-b5c6-1c184ec97d5a"
   },
   "source": [
    "3. Translation\n",
    "Finally, we can translate the multilingual text into a single language, such as English or Arabic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1974b2e9-dde1-452f-a07a-dfc918f75408",
   "metadata": {
    "id": "1974b2e9-dde1-452f-a07a-dfc918f75408",
    "outputId": "0d50674c-b294-41b5-d55f-b9a01c5b3df5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations into English:\n",
      "I love learning languages\n",
      "I love to learn new languages\n",
      "======================================================================================================================================================\n",
      "Translations into arabic:\n",
      "أحب تعلم اللغات\n",
      "أحب أن أتعلم لغات جديدة\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "# Initialize the Translator\n",
    "translator = Translator()\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = multilingual_text.split('.')\n",
    "\n",
    "# Translate each sentence into English\n",
    "translations_en = [translator.translate(sentence.strip(), src=detect_language(sentence), dest='en').text for sentence in sentences if sentence.strip()]\n",
    "\n",
    "translations_ar = [translator.translate(sentence.strip(), src=detect_language(sentence), dest='ar').text for sentence in sentences if sentence.strip()]\n",
    "\n",
    "# Display the translations\n",
    "print(\"Translations into English:\")\n",
    "for translation in translations_en:\n",
    "    print(translation)\n",
    "print(\"=\"*150)\n",
    "print(\"Translations into arabic:\")\n",
    "for translation in translations_ar:\n",
    "    print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22054df-401a-41b3-81c0-c5d61e082f51",
   "metadata": {
    "id": "f22054df-401a-41b3-81c0-c5d61e082f51"
   },
   "source": [
    "Explanation of Translation:\n",
    "Input Text: \"I love learning languages. J'adore apprendre de nouvelles langues.\"\n",
    "\n",
    "Process:\n",
    "The text is split into sentences.\n",
    "For each sentence, we detect the language and translate it into English and Arabic using googletrans.\n",
    "\n",
    "Output: Each sentence is translated into English and Arabic. The translation process makes it easier to handle multilingual content in a unified language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d714bf-3dd0-4b05-b3bb-cdb51a29028f",
   "metadata": {
    "id": "b9d714bf-3dd0-4b05-b3bb-cdb51a29028f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
